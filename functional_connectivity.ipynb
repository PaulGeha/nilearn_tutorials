{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating functional connectivity on a large dataset\n",
    "\n",
    "Extracting time series from a functional dataset is long, mainly due to heavy I/O operations. Fortunately, the Preprocessed Connectome Project, initiated by the 1000 Functional Connectomes Project (FCP) and International Neuroimaging Data-sharing Initiative (INDI), provides time-series extracted and pre-processed (filtering, movement regression) for this dataset. The main advantage is that time series are very lightweight.\n",
    "\n",
    "We use the `fetch_abide_pcp` fetcher and specify that we want the `rois_cc200` derivatives, which correspond to the timeseries extracted for the Craddock 2012 atlas.\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "* How to download the ABIDE dataset\n",
    "* How to estimate a functional connectome\n",
    "* How to use it to diagnose ASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['phenotypic', 'description', 'rois_cc200'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.datasets.func import fetch_abide_pcp\n",
    "\n",
    "\n",
    "abide = fetch_abide_pcp(n_subjects=None, quality_checked=True,\n",
    "                        band_pass_filtering=True, pipeline='cpac',\n",
    "                        derivatives='rois_cc200')\n",
    "abide.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectomes Estimation\n",
    "\n",
    "Connectivity is typically estimated using *correlation* between time series. Recent studies has shown that *partial correlation* could give better results. Different estimators can also be used to apply some regularization on the matrix coefficients. Nilearn's `ConnectivityMeasure` object (in the `nilearn.connectome` module) provides three types of connectivity matrix: *correlation*, *partial_correlation*, and *tangent* (a method developped in our laboratory). `ConnectivityMeasure` can also use any covariance estimator shipped by scikit-learn (`ShrunkCovariance`, `GraphLasso`).\n",
    "\n",
    "In a first time, we estimate the connectivity using default parameters. We check that we have one matrix per subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871, 200, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "\n",
    "conn_est = ConnectivityMeasure()\n",
    "conn_matrices = conn_est.fit_transform(abide.rois_cc200)\n",
    "conn_matrices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting connectivity matrix\n",
    "\n",
    "We visualize the mean connectivity matrix. This code is directly taken from a nilearn example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_matrix = np.mean(conn_matrices, axis=0)\n",
    "np.fill_diagonal(conn_matrix, 0)\n",
    "plt.imshow(conn_matrix, vmin=-1., vmax=1., cmap='RdBu_r')\n",
    "plt.colorbar()\n",
    "plt.title('ABIDE mean connectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting useful coefficients\n",
    "\n",
    "Connecitivity matrices are symmetric. As such, half of the coefficients are redundant. They can even impact the results of some predictors. In order to \"extract\" these coefficients, we want to use a mask. `numpy.tril` function can help us with this task. However, using masking is hazardous without a good knowledge of numpy. Fortunately, nilearn provides a function to do this automatically and efficiently: `nilearn.connectome.sym_to_vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871, 20100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.connectome import sym_to_vec\n",
    "\n",
    "\n",
    "X = sym_to_vec(conn_matrices)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting up cross-validation\n",
    "\n",
    "Getting reliable prediction results require to predict on unseen data. Cross-validation consists in leaving out a part of the dataset (testing set) to validate the model learnt on the remaining of the dataset (training set). Scikit-learn has all the utils necessary to do automatic cross-validation. In the case of ABIDE, we have a very heterogenous dataset and we want the sets to be balanced in term of acquisition sites and condition. We use a stratified cross-validation method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "id = [str(sid) + str(dx) for sid, dx in abide.phenotypic[['SITE_ID', 'DX_GROUP']]]\n",
    "cv = StratifiedShuffleSplit(id, n_iter=10, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using Support Vector Classifier\n",
    "\n",
    "Now that we have shown how to estimate a connectome and extract the interesting coefficients, we will see how to use them to diagnose ASD vs healthy individuals.\n",
    "\n",
    "For that purpose, we use a Support Vector Machine. This is one of the most simple classifiers. We use the default parameters in a first time and look at classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64195402298850579"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y = abide.phenotypic['DX_GROUP']\n",
    "predictor = LinearSVC(C=0.01)\n",
    "np.mean(cross_val_score(predictor, X, y, cv=cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a ROC curve\n",
    "\n",
    "XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring other methods and parameters\n",
    "\n",
    "So far, we built a basic prediction procedure without tuning the parameters. Now we use for loops to explore several options. Note that the imbrication of the steps allow us to re-use connectivity matrix computed in the first loop for the different predictors. The same result can be achieved using nilearn's caching capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.67183908046\n",
      "correlation LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.545402298851\n",
      "correlation Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) -5.34762742546e-05\n",
      "partial correlation LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.640229885057\n",
      "partial correlation LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.540229885057\n",
      "partial correlation Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) -5.34762742546e-05\n",
      "tangent LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.686781609195\n",
      "tangent LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,\n",
      "     verbose=0) 0.522988505747\n",
      "tangent Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) -5.34762742546e-05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "measures = ['correlation', 'partial correlation', 'tangent']\n",
    "predictors = [LinearSVC(C=0.01), LinearSVC(C=0.01, penalty='l1', dual=False), Lasso()]\n",
    "\n",
    "for measure in measures:\n",
    "    conn_est = ConnectivityMeasure(kind=measure)\n",
    "    conn_matrices = conn_est.fit_transform(abide.rois_cc200)\n",
    "    X = sym_to_vec(conn_matrices)\n",
    "    for predictor in predictors:\n",
    "        print(measure, str(predictor), np.mean(cross_val_score(predictor, X, y, cv=cv)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
